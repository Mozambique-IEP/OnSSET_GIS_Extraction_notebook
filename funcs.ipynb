{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import fiona\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from rasterstats import zonal_stats\n",
    "import rasterio\n",
    "import rasterio.fill\n",
    "from shapely.geometry import shape, mapping\n",
    "import json\n",
    "from earthpy import clip\n",
    "import earthpy.spatial as es\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import gdal\n",
    "import datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import scipy.spatial\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_raster(name, method, clusters):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(filedialog.askopenfilename(filetypes = ((\"rasters\",\"*.tif\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Elevation and Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_elevation_and_slope(name, method, clusters, workspace,crs):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(filedialog.askopenfilename(filetypes = ((\"rasters\",\"*.tif\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "\n",
    "    gdal.Warp(workspace + r\"\\dem.tif\",raster.name,dstSRS=crs)\n",
    "\n",
    "    def calculate_slope(DEM):\n",
    "        gdal.DEMProcessing(workspace + r'\\slope.tif', DEM, 'slope')\n",
    "        with rasterio.open(workspace + r'\\slope.tif') as dataset:\n",
    "            slope=dataset.read(1)\n",
    "        return slope\n",
    "\n",
    "    slope=calculate_slope(workspace + r\"\\dem.tif\")\n",
    "\n",
    "    slope = rasterio.open(workspace + r'\\slope.tif')\n",
    "    gdal.Warp(workspace + r'\\slope_4326.tif',slope.name,dstSRS='EPSG:4326')\n",
    "    slope_4326 = rasterio.open(workspace + r'\\slope_4326.tif')\n",
    "\n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        slope_4326.name,\n",
    "        stats=[\"majority\"],\n",
    "        prefix=\"sl_\", all_touched = True, geojson_out=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalizing rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalizing_rasters(workspace, clusters, crs):\n",
    "    clusters[\"Placeholder\"] = 0\n",
    "    output = workspace + r'\\placeholder.geojson'\n",
    "    with open(output, \"w\") as dst:\n",
    "        collection = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": list(clusters)}\n",
    "        dst.write(json.dumps(collection))\n",
    "  \n",
    "    clusters = gpd.read_file(output)\n",
    "    os.remove(output)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_for_vectors(workspace, clusters, crs):   \n",
    "    clusters.crs = {'init' :'epsg:4326'}\n",
    "    clusters = clusters.to_crs({ 'init': crs}) \n",
    "    points = clusters.copy()\n",
    "    points[\"geometry\"] = points[\"geometry\"].centroid\n",
    "    points.to_file(workspace + r'\\clusters_cp.shp', driver='ESRI Shapefile')\n",
    "    print(datetime.datetime.now())    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_lines(name, admin, crs, workspace, clusters):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    #lines_f=fiona.open(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    lines=gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "\n",
    "    lines_clip = clip.clip_shp(lines, admin)\n",
    "    lines_clip.crs = {'init' :'epsg:4326'}\n",
    "    lines_proj=lines_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    lines_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    line = fiona.open(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    firstline = line.next()\n",
    "\n",
    "    schema = {'geometry' : 'Point', 'properties' : {'id' : 'int'},}\n",
    "    with fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\", \"w\", \"ESRI Shapefile\", schema) as output:\n",
    "        for lines in line:\n",
    "            if lines[\"geometry\"] is not None:\n",
    "                first = shape(lines['geometry'])\n",
    "                length = first.length\n",
    "                for distance in range(0,int(length),100):\n",
    "                    point = first.interpolate(distance)\n",
    "                    output.write({'geometry' :mapping(point), 'properties' : {'id':1}})\n",
    "\n",
    "    lines_f = fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\")\n",
    "    lines = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in lines_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "\n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000\n",
    "\n",
    "    a = vector_overlap(lines, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_points(name, admin, crs, workspace, clusters):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    #points_f=fiona.open(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    points=gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "\n",
    "    points_clip = clip.clip_shp(points, admin)\n",
    "    points_clip.crs = {'init' :'epsg:4326'}\n",
    "    points_proj=points_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    points_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    points_f = fiona.open(workspace + r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points2 = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "    \n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000\n",
    "\n",
    "    a = vector_overlap(points, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing hydro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_hydro(name, admin, crs, workspace, clusters, points):\n",
    "\n",
    "    points_clip = clip.clip_shp(points, admin)\n",
    "    points_clip.crs = {'init' :'epsg:4326'}\n",
    "    points_proj=points_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    points_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    points_f = fiona.open(workspace + r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points2 = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    #geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "    #s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    #s1_arr = np.array(s1)\n",
    "    #s1_arr = points.to_numpy()\n",
    "    \n",
    "    \n",
    "    geoms1 = []\n",
    "    for each in points_f:\n",
    "        geoms1.append(each[\"properties\"])\n",
    "        geoms1.append(each[\"geometry\"])\n",
    "    \n",
    "    s1_arr = np.array(geoms1)\n",
    "    \n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000\n",
    "\n",
    "    a = vector_overlap(points, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #from shapely.ops import nearest_points\n",
    "\n",
    "    #hydro_clip = clip.clip_shp(hydro, admin)\n",
    "    #hydro.crs = {'init' :'epsg:4326'}\n",
    "    #hydro_proj=hydro.to_crs({ 'init': crs})\n",
    "\n",
    "    #hydro_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    #points = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    #points2 = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "    #hydro_proj = clip.clip_shp(hydro, admin)\n",
    "    \n",
    "    #pts3 = hydro_proj.geometry.unary_union\n",
    "    #points['id'] = np.arange(len(points))\n",
    "\n",
    "    #def near(point, point2=pts3):\n",
    "    #    nearest = hydro_proj.geometry == nearest_points(point, point2)[1]\n",
    "    #    return hydro_proj[nearest].get_values()[0]\n",
    "    \n",
    "    #x = unit\n",
    "\n",
    "    #if x is 'MW':\n",
    "    #    clusters['hydropower'] = clusters.apply(lambda row: near(row.geometry)[points.columns.get_loc(x)]*1000, axis=1)\n",
    "    #elif x is 'kW':\n",
    "    #    clusters['hydropower'] = clusters.apply(lambda row: near(row.geometry)[points.columns.get_loc(x)], axis=1)\n",
    "    #else:\n",
    "    #    clusters['hydropower'] = clusters.apply(lambda row: near(row.geometry)[points.columns.get_loc(x)]/1000, axis=1)\n",
    "\n",
    "    #clusters['hydrodist'] = clusters.geometry.apply(lambda g: points.distance(g).min())\n",
    "    #clusters['hydropowerFID'] = clusters.apply(lambda row: near(row.geometry)[points.columns.get_loc('id')], axis=1)\n",
    "    #return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditioning(clusters, workspace, x):\n",
    "    clusters = clusters.to_crs({ 'init': 'epsg:4326'}) \n",
    "\n",
    "    clusters.rename(columns={\"NightLight\": \"NightLights\", popunit.value : \"Pop\", \"GridCellAr\":\"GridCellArea\"})\n",
    "\n",
    "    if \"landcover_\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"landcover_\": \"LandCover\"})\n",
    "    else:\n",
    "        raise Exception('The landcover column is missing, this is a mandatory column rerun cell X')\n",
    "\n",
    "    if \"elevation_\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"elevation_\": \"Elevation\"})\n",
    "    else:\n",
    "        raise Exception('The elevation column is missing, this is a mandatory column rerun cell X')    \n",
    "\n",
    "    if \"slope_majo\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"slope_majo\": \"Slope\"})\n",
    "    else:\n",
    "        raise Exception('The slope column is missing, this is a mandatory column rerun cell X')  \n",
    "\n",
    "    if \"ghi_mean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"ghi_mean\": \"GHI\"})\n",
    "    else:\n",
    "        raise Exception('The solar column is missing, this is a mandatory column rerun cell X')  \n",
    "\n",
    "    if \"traveltime\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"traveltime\": \"TravelHours\"})\n",
    "    else:\n",
    "        raise Exception('The traveltime column is missing, this is a mandatory column rerun cell X')\n",
    "\n",
    "    if \"wind_mean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"wind_mean\": \"WindVel\"})\n",
    "    else:\n",
    "        raise Exception('The wind speed column is missing, this is a mandatory column rerun cell X')\n",
    "\n",
    "    if \"customdemand\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"customdemand\": \"ResidentialDemandTierCustom\"})\n",
    "    else:\n",
    "        clusters[\"ResidentialDemandTierCustom\"] = 0\n",
    "\n",
    "    if \"Substation\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Substation\": \"SubstationDist\"})\n",
    "    else:\n",
    "        clusters[\"SubstationDist\"] = 99\n",
    "\n",
    "    if \"Existing_H\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Existing_H\": \"CurrentHVLineDist\"})  \n",
    "    else:\n",
    "        clusters[\"CurrentHVLineDist\"] = 0    \n",
    "\n",
    "    if \"Planned_HV\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Planned_HV\": \"PlannedHVLineDist\"})\n",
    "    elif \"CurrentHVLineDist\" not in clusters:\n",
    "        clusters[\"PlannedHVLineDist\"] = 0\n",
    "    else:\n",
    "        clusters[\"PlannedHVLineDist\"] = clusters[\"CurrentHVLineDist\"]  \n",
    "\n",
    "    if \"Existing_M\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Existing_M\": \"CurrentMVLineDist\"})\n",
    "    else:\n",
    "        clusters[\"CurrentMVLineDist\"] = 0\n",
    "\n",
    "    if \"Planned_MV\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Planned_MV\": \"PlannedMVLineDist\"})\n",
    "    elif \"CurrentMVLineDist\" not in clusters:\n",
    "        clusters[\"PlannedMVLineDist\"] = 0\n",
    "    else:\n",
    "        clusters[\"PlannedMVLineDist\"] = clusters[\"CurrentMVLineDist\"]    \n",
    "\n",
    "    if \"RoadDist\" not in clusters:\n",
    "        clusters[\"RoadDist\"] = 99\n",
    "\n",
    "    if \"Transforme\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Transforme\": \"TransformerDist\"})\n",
    "    else:\n",
    "        clusters[\"TransformerDist\"] = 0\n",
    "\n",
    "    if \"hydropow_1\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"hydropow_1\": \"HydropowerFID\"})\n",
    "    else:\n",
    "        clusters[\"HydropowerFID\"] = 0\n",
    "\n",
    "    if \"hydrodist\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"hydrodist\": \"HydropowerDist\"})\n",
    "    else:\n",
    "        clusters[\"HydropowerDist\"] = 99\n",
    "\n",
    "    if \"hydropower\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"hydropower\": \"Hydropower\"})\n",
    "    else:\n",
    "        clusters[\"Hydropower\"] = 0\n",
    "\n",
    "    clusters[\"IsUrban\"] = 0\n",
    "    clusters[\"PerCapitaDemand\"] = 0\n",
    "    clusters[\"HealthDemand\"] = 0    \n",
    "    clusters[\"EducationDemand\"] = 0\n",
    "    clusters[\"AgriDemand\"] = 0\n",
    "    clusters[\"CommercialDemand\"] = 0\n",
    "    clusters[\"Conflict\"] = 0\n",
    "    clusters[\"ElectrificationOrder\"] = 0\n",
    "    clusters[\"ResidentialDemandTier1\"] = 7.74\n",
    "    clusters[\"ResidentialDemandTier2\"] = 43.8\n",
    "    clusters[\"ResidentialDemandTier3\"] = 160.6\n",
    "    clusters[\"ResidentialDemandTier4\"] = 423.4\n",
    "    clusters[\"ResidentialDemandTier5\"] = 598.6\n",
    "    clusters[\"X_deg\"] = clusters.geometry.centroid.x\n",
    "    clusters[\"Y_deg\"] = clusters.geometry.centroid.y\n",
    "    clusters.to_file(workspace + r\"\\output.shp\", driver='ESRI Shapefile')\n",
    "    clusters.to_file(workspace + r\"\\output.csv\", driver='CSV')\n",
    "    print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
